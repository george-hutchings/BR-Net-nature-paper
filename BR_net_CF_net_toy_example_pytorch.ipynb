{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFyz9zm0NNBj",
        "outputId": "66221840-9481-48fc-aca7-7cc237ad5e3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-09 20:49:57.857481: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense, Flatten, Input, Layer, MaxPooling2D, Conv2D, Reshape\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "from visualization_functions import *\n",
        "import itertools\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "import time\n",
        "\n",
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import dcor\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCtnKdW7NNBj"
      },
      "source": [
        "# Simulating Synthetic Images\n",
        "The training images of two groups are simulated. Each image contains 4 Gaussian distribution density functions. Let the 4 standard deviations be\n",
        "\n",
        "|  $\\sigma_A$ | $\\sigma_B$  |\n",
        "\n",
        "|  $\\sigma_B$ | $\\sigma_A$  |\n",
        "\n",
        "The 4 Gaussians are constructed such that\n",
        "\n",
        "1. height of the two diagonal Gaussians $\\sigma_A$ is linked to a factor of interest $mf$ (e.g. true effect between two classes)\n",
        "2. height of the two off-diagonal Gaussians $\\sigma_B$ is linked to two different confounding factors $cf$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "G9uO_fA9NNBk",
        "outputId": "d230fb53-d0a2-4767-acde-6bd5ac48e170"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Simulate Data\n",
        "np.random.seed(1)\n",
        "\n",
        "N = 2**10 # number of subjects in a group\n",
        "labels = np.zeros((N*2,))\n",
        "labels[N:] = 1\n",
        "\n",
        "# 2 confounding effects between 2 groups\n",
        "cf = np.zeros((N*2,))\n",
        "cf[:N] = np.random.uniform(1,4,size=N)\n",
        "cf[N:] = np.random.uniform(3,6,size=N)\n",
        "\n",
        "# 2 major effects between 2 groups\n",
        "mf = np.zeros((N*2,))\n",
        "mf[:N] = np.random.uniform(1,4,size=N)\n",
        "mf[N:] = np.random.uniform(3,6,size=N)\n",
        "\n",
        "# simulate images\n",
        "x = np.zeros((N*2,32,32,1))\n",
        "y = np.zeros((N*2,))\n",
        "y[N:] = 1\n",
        "for i in range(N*2):\n",
        "    x[i,:16,:16,0] = gkern(kernlen=16, nsig=5)*mf[i]\n",
        "    x[i,16:,:16,0] = gkern(kernlen=16, nsig=5)*cf[i]\n",
        "    x[i,:16,16:,0] = gkern(kernlen=16, nsig=5)*cf[i]\n",
        "    x[i,16:,16:,0] = gkern(kernlen=16, nsig=5)*mf[i]\n",
        "    x[i] = x[i] + np.random.normal(0,0.01,size=(32,32,1))\n",
        "\n",
        "plt.imshow(x[1,:,:,0],cmap='gray')\n",
        "plt.colorbar()\n",
        "#plt.title(\"a synthetic training image\");\n",
        "plt.xticks(np.arange(0), ())\n",
        "plt.yticks(np.arange(0), ())\n",
        "plt.savefig('synthetic_sample.jpg', format='jpg', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lhCdJbBNNBl"
      },
      "source": [
        "# Training BR-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bM4HxpBiNNBl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Adversarial loss for squared correlation\n",
        "def lossb_calc(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = torch.mean(x)\n",
        "    my = torch.mean(y)\n",
        "    xm, ym = x - mx, y - my\n",
        "    r_num = torch.sum(torch.mul(xm, ym))\n",
        "    r_den = torch.sqrt(torch.mul(torch.sum(torch.square(xm)), torch.sum(torch.square(ym)))) + 1e-5\n",
        "    r = r_num / r_den\n",
        "\n",
        "    r = torch.clamp(r, min=-1.0, max=1.0)\n",
        "    return -torch.square(r)\n",
        "\n",
        "# Adversarial loss for mse\n",
        "def inv_mse(y_true, y_pred):\n",
        "    mse_value = torch.sum(torch.square(y_true - y_pred))\n",
        "    return -mse_value\n",
        "\n",
        "lossp_calc = torch.nn.BCELoss()\n",
        "#lossb_calc = torch.nn.MSELoss()\n",
        "\n",
        "lossb_adverseral_calc = torch.nn.MSELoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0E30Z1DiNNBl"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, label, confounders):\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "        self.confounders = confounders\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.label[index], self.confounders[index]\n",
        "    def __len__(self):\n",
        "        return len(self.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xMbZa2EnNNBl"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "train_propotion = 0.8\n",
        "i = int(N*2*train_propotion)\n",
        "\n",
        "#convert data to float32\n",
        "xx = x.astype(np.float32)\n",
        "yy = y.astype(np.float32)\n",
        "cfcf = cf.astype(np.float32)\n",
        "\n",
        "train_x = xx[:i]\n",
        "valid_x = xx[i:]\n",
        "\n",
        "trainset = MyDataset(xx[:i],yy[:i], cfcf[:i])\n",
        "valset = MyDataset(xx[i:],yy[i:], cfcf[i:])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size, shuffle=True, pin_memory=True,num_workers=1)\n",
        "valid_loader = torch.utils.data.DataLoader(valset, batch_size, shuffle=False, pin_memory=True,num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v1LootliNNBl"
      },
      "outputs": [],
      "source": [
        "class modelF1(nn.Module):\n",
        "    \"\"\"\n",
        "    feature predictor\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(modelF1, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 2, kernel_size=3, padding=0),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(2, 4, kernel_size=3, padding=0),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(4, 8, kernel_size=3, padding=0),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class modelB1(nn.Module):\n",
        "    \"\"\"\n",
        "    bias predictor\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(modelB1, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Linear(32, 16),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(16, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class modelP1(nn.Module):\n",
        "    \"\"\"\n",
        "    predictor\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(modelP1, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Linear(32, 16),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20rg-ZmWNNBm",
        "outputId": "3439d1be-3f5f-4f04-d38c-db51b8710eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is available. Training on GPU\n"
          ]
        }
      ],
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available. Training on CPU')\n",
        "else:\n",
        "    print('CUDA is available. Training on GPU')\n",
        "\n",
        "device = torch.device(\"cuda\" if train_on_gpu else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rQakDhK9NNBm"
      },
      "outputs": [],
      "source": [
        "\n",
        "modelF = modelF1()\n",
        "modelP = modelP1()\n",
        "modelB = modelB1()\n",
        "\n",
        "modelF = torch.nn.DataParallel(modelF).to(device) # send tensor to device\n",
        "modelP = torch.nn.DataParallel(modelP).to(device) # send tensor to device\n",
        "modelB = torch.nn.DataParallel(modelB).to(device) # send tensor to device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RoE6fZ2dNNBm"
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.Adam(list(modelF.parameters())+ list(modelP.parameters())+list(modelB.parameters()), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNrLv_V5NNBm",
        "outputId": "a06cf50e-2a3d-43bc-cc66-3b3d06322337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20000 Train loss: 0.7089 Valid loss: 1.2867 Train lossb: -0.0414 Valid lossb: -0.0478 Time: 0m 1s\n",
            "Epoch 101/20000 Train loss: 0.6618 Valid loss: 1.9601 Train lossb: -0.0108 Valid lossb: -0.0506 Time: 0m 0s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m modelP\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     21\u001b[0m modelB\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, b \u001b[38;5;129;01min\u001b[39;00m train_loader: \u001b[38;5;66;03m# b is bias\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m#no control group as easier to train\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     25\u001b[0m         y, b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(y,\u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39munsqueeze(b,\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1318\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1318\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \n\u001b[1;32m   1323\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1443\u001b[0m     \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[1;32m   1445\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_loss_list = []\n",
        "valid_loss_list = []\n",
        "train_lossb_list = []\n",
        "valid_lossb_list = []\n",
        "\n",
        "\n",
        "\n",
        "epochs = 20000\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    train_lossb = 0.0\n",
        "    valid_lossb = 0.0\n",
        "\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    modelF.train()\n",
        "    modelP.train()\n",
        "    modelB.train()\n",
        "    for x, y, b in train_loader: # b is bias\n",
        "            #no control group as easier to train\n",
        "            x = x.permute(0,3,1,2)\n",
        "            y, b = torch.unsqueeze(y,1), torch.unsqueeze(b,1)\n",
        "            x,y,b = x.to(device), y.to(device), b.to(device)\n",
        "            F_pred = modelF(x)\n",
        "            y_pred = modelP(F_pred)\n",
        "            b_pred = modelB(F_pred)\n",
        "\n",
        "            #no adversarial loss for the ctrl group\n",
        "            lossp = lossp_calc(y_pred, y)\n",
        "            lossb = lossb_calc(b_pred, b)\n",
        "            lossb_adverseral = lossb_adverseral_calc(b_pred, b)\n",
        "\n",
        "            train_loss += lossp.item()\n",
        "            train_lossb += lossb.item()\n",
        "\n",
        "            opt.zero_grad(True)\n",
        "            # Step 1\n",
        "            (lossp - lossb).backward(retain_graph=True, inputs = tuple(modelF.parameters())) #want to maximise lossb\n",
        "\n",
        "            # Step 2\n",
        "            (lossb_adverseral).backward(retain_graph=True, inputs = tuple(modelB.parameters())) # want to minimise lossb_adverseral (b is - corr^2 or mse) this is adverseral part\n",
        "\n",
        "            # Step 3\n",
        "            (lossp).backward(inputs= tuple(modelP.parameters()))\n",
        "\n",
        "            opt.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    modelF.eval()\n",
        "    modelP.eval()\n",
        "    modelB.eval()\n",
        "    with torch.no_grad(): #to increase the validation process uses less memory\n",
        "        for x1, y1, b1 in valid_loader:\n",
        "            x1 = x1.permute(0,3,1,2)\n",
        "            y1, b1 = torch.unsqueeze(y1,1), torch.unsqueeze(b1,1)\n",
        "            x1,y1,b1 = x1.to(device), y1.to(device), b1.to(device)\n",
        "\n",
        "            F_pred = modelF(x1)\n",
        "            y_pred = modelP(F_pred)\n",
        "            b_pred = modelB(F_pred)\n",
        "\n",
        "            lossp = lossp_calc(y_pred, y1)\n",
        "            lossb = lossb_calc(b_pred, b1)\n",
        "\n",
        "            valid_loss += lossp.item()\n",
        "            valid_lossb += lossb.item()\n",
        "\n",
        "            valid_loss += lossp.item()\n",
        "            valid_lossb += lossb.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    valid_loss /= len(valid_loader)\n",
        "    train_lossb /= len(train_loader)\n",
        "    valid_lossb /= len(valid_loader) # hence this is the per batch loss\n",
        "\n",
        "\n",
        "    train_loss_list += [train_loss]\n",
        "    valid_loss_list += [valid_loss]\n",
        "    train_lossb_list += [train_lossb]\n",
        "    valid_lossb_list += [valid_lossb]\n",
        "\n",
        "\n",
        "    if (epoch%100==0):\n",
        "      #print time per epoch\n",
        "      time_elapsed = time.time() - since\n",
        "      print('Epoch {}/{}'.format(epoch+1, epochs),\n",
        "            'Train loss: {:.4f}'.format(train_loss),\n",
        "            'Valid loss: {:.4f}'.format(valid_loss),\n",
        "            'Train lossb: {:.4f}'.format(train_lossb),\n",
        "            'Valid lossb: {:.4f}'.format(valid_lossb),\n",
        "            'Time: {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEiDsJkfNNBm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have the following variables defined:\n",
        "# train_loss, valid_loss, train_lossb, valid_lossb\n",
        "\n",
        "\n",
        "\n",
        "# Plotting loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(train_loss_list), label='Train Loss')\n",
        "plt.plot(np.array(valid_loss_list), label='Valid Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting lossb\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(train_lossb_list), label='Train Lossb')\n",
        "plt.plot(np.array(valid_lossb_list), label='Valid Lossb')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Lossb')\n",
        "plt.title('Lossb')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNWZcg11NNBn"
      },
      "source": [
        "# Visualizing the Association between Features and Confounder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x = xx[:i]\n",
        "valid_x = xx[i:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIlsdmgNNBn"
      },
      "outputs": [],
      "source": [
        "for x in [train_x, valid_x]:\n",
        "\n",
        "  x = torch.tensor(x, dtype=torch.float32).to(device)\n",
        "  x = x.permute(0,3,1,2)\n",
        "\n",
        "\n",
        "  # Get the gradients of the output with respect to the input\n",
        "  x.requires_grad_()\n",
        "\n",
        "  modelF.eval()\n",
        "  modelP.eval()\n",
        "  output = modelP(modelF(x)).sum(dim=0)\n",
        "  modelF.zero_grad()\n",
        "  modelP.zero_grad()\n",
        "  output.backward()\n",
        "\n",
        "  # Compute the saliency map\n",
        "  saliency_map = x.grad.abs().squeeze().mean(dim=0).cpu().numpy()\n",
        "\n",
        "  # Normalize the saliency map\n",
        "  saliency_map /= saliency_map.max()\n",
        "\n",
        "  # Plot the original image and the saliency map\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(x[0,0,:,:].detach().cpu().numpy())\n",
        "  plt.title('Original Image')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(saliency_map)\n",
        "  plt.title('Saliency Map')\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dcorr loss\n",
        "def dcor_loss_calc(X, Y):\n",
        "    \"\"\" Compute the distance correlation function using PyTorch tensors\"\"\"\n",
        "\n",
        "\n",
        "    #n = X.size(dim=1)\n",
        "  \n",
        "    # Compute distance matrices\n",
        "    a = torch.cdist(X.unsqueeze(2), X.unsqueeze(2))\n",
        "    b = torch.cdist(Y.unsqueeze(2), Y.unsqueeze(2)) \n",
        "    # assuming X, Y is a vector we unsqueeze\n",
        "\n",
        "\n",
        "\n",
        "    # Compute double centered distance matrices\n",
        "    A = a - a.mean(dim=1, keepdim=True) - a.mean(dim=2, keepdim=True) + a.mean(dim=(1,2), keepdim=True)\n",
        "    B = b - b.mean(dim=1, keepdim=True) - b.mean(dim=2, keepdim=True) + b.mean(dim=(1,2), keepdim=True)\n",
        "\n",
        "    # Compute distance covariance and variance\n",
        "    dCovXY = (A * B).sum(dim=(1,2)) #/ (n**2)\n",
        "    dVarX = (A**2).sum(dim=(1,2)) #/ (n**2)\n",
        "    dVarY = (B**2).sum(dim=(1,2)) #/ (n**2)\n",
        "\n",
        "    # Compute distance correlation\n",
        "    dCorXY = dCovXY / (dVarX * dVarY).sqrt()\n",
        "\n",
        "    return dCorXY.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for layer in modelF.children():\n",
        "   if hasattr(layer, 'reset_parameters'):\n",
        "       layer.reset_parameters()\n",
        "\n",
        "for layer in modelP.children():\n",
        "    if hasattr(layer, 'reset_parameters'):\n",
        "         layer.reset_parameters()\n",
        "\n",
        "for layer in modelB.children():\n",
        "    if hasattr(layer, 'reset_parameters'):\n",
        "         layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YtTcPQYdNNBo"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m modelF\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     20\u001b[0m modelP\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, b \u001b[38;5;129;01min\u001b[39;00m train_loader: \u001b[38;5;66;03m# b is bias\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m#no control group as easier to train\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         x,y,b \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device), b\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_loss_list = []\n",
        "valid_loss_list = []\n",
        "train_lossb_list = []\n",
        "valid_lossb_list = []\n",
        "\n",
        "lam = 1\n",
        "\n",
        "epochs = 20000\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    train_lossb = 0.0\n",
        "    valid_lossb = 0.0\n",
        "\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    modelF.train()\n",
        "    modelP.train()\n",
        "    for x, y, b in train_loader: # b is bias\n",
        "            #no control group as easier to train\n",
        "            x,y,b = x.to(device), y.to(device), b.to(device)\n",
        "            x = x.permute(0,3,1,2)\n",
        "            y, b = torch.unsqueeze(y,1), torch.unsqueeze(b,1)\n",
        "            F_pred = modelF(x)\n",
        "            y_pred = modelP(F_pred)\n",
        "            print(x.size(), y.size(), b.size())\n",
        "            print(F_pred.size(), y_pred.size())\n",
        "            print(y.min(), y.max())\n",
        "            print(y_pred.min(), y_pred.max())\n",
        "\n",
        "            #no adversarial loss for the ctrl group\n",
        "            lossp = lossp_calc(y_pred, y)\n",
        "            lossb = dcor_loss_calc(F_pred, b)\n",
        "\n",
        "            train_loss += lossp.item()\n",
        "            train_lossb += lossb.item()\n",
        "\n",
        "            opt.zero_grad(True)\n",
        "            (lossp + lam*lossb).backward() \n",
        "            opt.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    modelF.eval()\n",
        "    modelP.eval()\n",
        "    with torch.no_grad(): #to increase the validation process uses less memory\n",
        "        for x1, y1, b1 in valid_loader:\n",
        "            x1,y1,b1 = x1.to(device), y1.to(device), b1.to(device)\n",
        "            x1 = x1.permute(0,3,1,2)\n",
        "            y1, b1 = torch.unsqueeze(y1,1), torch.unsqueeze(b1,1)\n",
        "            \n",
        "            F_pred = modelF(x1)\n",
        "            y_pred = modelP(F_pred)\n",
        "\n",
        "            lossp = lossp_calc(y_pred, y1)\n",
        "            lossb = dcor_loss_calc(F_pred, b1)\n",
        "\n",
        "            valid_loss += lossp.item()\n",
        "            valid_lossb += lossb.item()\n",
        "\n",
        "            valid_loss += lossp.item()\n",
        "            valid_lossb += lossb.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    valid_loss /= len(valid_loader)\n",
        "    train_lossb /= len(train_loader)\n",
        "    valid_lossb /= len(valid_loader) # hence this is the per batch loss\n",
        "\n",
        "\n",
        "    train_loss_list += [train_loss]\n",
        "    valid_loss_list += [valid_loss]\n",
        "    train_lossb_list += [train_lossb]\n",
        "    valid_lossb_list += [valid_lossb]\n",
        "\n",
        "\n",
        "    if (epoch%100==0):\n",
        "      #print time per epoch\n",
        "      time_elapsed = time.time() - since\n",
        "      print('Epoch {}/{}'.format(epoch+1, epochs),\n",
        "            'Train loss: {:.4f}'.format(train_loss),\n",
        "            'Valid loss: {:.4f}'.format(valid_loss),\n",
        "            'Train lossb: {:.4f}'.format(train_lossb),\n",
        "            'Valid lossb: {:.4f}'.format(valid_lossb),\n",
        "            'Time: {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
