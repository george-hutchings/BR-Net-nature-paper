{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XFyz9zm0NNBj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gkern(kernlen=21, nsig=3):\n",
        "    import numpy\n",
        "    import scipy.stats as st\n",
        "\n",
        "    \"\"\"Returns a 2D Gaussian kernel array.\"\"\"\n",
        "\n",
        "    interval = (2*nsig+1.)/(kernlen)\n",
        "    x = numpy.linspace(-nsig-interval/2., nsig+interval/2., kernlen+1)\n",
        "    kern1d = numpy.diff(st.norm.cdf(x))\n",
        "    kernel_raw = numpy.sqrt(numpy.outer(kern1d, kern1d))\n",
        "    kernel = kernel_raw/kernel_raw.sum()\n",
        "    return kernel"
      ],
      "metadata": {
        "id": "nFYYVvgMAaY1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCtnKdW7NNBj"
      },
      "source": [
        "# Simulating Synthetic Images\n",
        "The training images of two groups are simulated. Each image contains 4 Gaussian distribution density functions. Let the 4 standard deviations be\n",
        "\n",
        "|  $\\sigma_A$ | $\\sigma_B$  |\n",
        "\n",
        "|  $\\sigma_B$ | $\\sigma_A$  |\n",
        "\n",
        "The 4 Gaussians are constructed such that\n",
        "\n",
        "1. height of the two diagonal Gaussians $\\sigma_A$ is linked to a factor of interest $mf$ (e.g. true effect between two classes)\n",
        "2. height of the two off-diagonal Gaussians $\\sigma_B$ is linked to two different confounding factors $cf$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "G9uO_fA9NNBk",
        "outputId": "fed8d8e0-ccc7-4891-c11b-a0c521025402"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Simulate Data\n",
        "np.random.seed(1)\n",
        "\n",
        "N = 2**10 # number of subjects in a group\n",
        "labels = np.zeros((N*2,))\n",
        "labels[N:] = 1\n",
        "\n",
        "# 2 confounding effects between 2 groups\n",
        "cf = np.zeros((N*2,))\n",
        "cf[:N] = np.random.uniform(1,4,size=N)\n",
        "cf[N:] = np.random.uniform(3,6,size=N)\n",
        "\n",
        "# 2 major effects between 2 groups\n",
        "mf = np.zeros((N*2,))\n",
        "mf[:N] = np.random.uniform(1,4,size=N)\n",
        "mf[N:] = np.random.uniform(3,6,size=N)\n",
        "\n",
        "# simulate images\n",
        "x = np.zeros((N*2,32,32,1))\n",
        "y = np.zeros((N*2,))\n",
        "y[N:] = 1\n",
        "for i in range(N*2):\n",
        "    x[i,:16,:16,0] = gkern(kernlen=16, nsig=5)*mf[i]\n",
        "    x[i,16:,:16,0] = gkern(kernlen=16, nsig=5)*cf[i]\n",
        "    x[i,:16,16:,0] = gkern(kernlen=16, nsig=5)*cf[i]\n",
        "    x[i,16:,16:,0] = gkern(kernlen=16, nsig=5)*mf[i]\n",
        "    x[i] = x[i] + np.random.normal(0,0.01,size=(32,32,1))\n",
        "\n",
        "plt.imshow(x[1,:,:,0],cmap='gray')\n",
        "plt.colorbar()\n",
        "#plt.title(\"a synthetic training image\");\n",
        "plt.xticks(np.arange(0), ())\n",
        "plt.yticks(np.arange(0), ())\n",
        "plt.savefig('synthetic_sample.jpg', format='jpg', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lhCdJbBNNBl"
      },
      "source": [
        "# Training BR-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bM4HxpBiNNBl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Adversarial loss for squared correlation\n",
        "def lossb_calc(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = torch.mean(x)\n",
        "    my = torch.mean(y)\n",
        "    xm, ym = x - mx, y - my\n",
        "    r_num = torch.sum(torch.mul(xm, ym))\n",
        "    r_den = torch.sqrt(torch.mul(torch.sum(torch.square(xm)), torch.sum(torch.square(ym)))) + 1e-5\n",
        "    r = r_num / r_den\n",
        "\n",
        "    r = torch.clamp(r, min=-1.0, max=1.0)\n",
        "    return -torch.square(r)\n",
        "\n",
        "# Adversarial loss for mse\n",
        "def inv_mse(y_true, y_pred):\n",
        "    mse_value = torch.sum(torch.square(y_true - y_pred))\n",
        "    return -mse_value\n",
        "\n",
        "lossp_calc = torch.nn.BCELoss()\n",
        "#lossb_calc = torch.nn.MSELoss()\n",
        "\n",
        "lossb_adverseral_calc = torch.nn.MSELoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0E30Z1DiNNBl"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, label, confounders):\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "        self.confounders = confounders\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.label[index], self.confounders[index]\n",
        "    def __len__(self):\n",
        "        return len(self.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xMbZa2EnNNBl"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "train_propotion = 0.8\n",
        "i = int(N*2*train_propotion)\n",
        "\n",
        "#convert data to float32\n",
        "xx = x.astype(np.float32)\n",
        "yy = y.astype(np.float32)\n",
        "cfcf = cf.astype(np.float32)\n",
        "\n",
        "train_x = xx[:i]\n",
        "valid_x = xx[i:]\n",
        "\n",
        "trainset = MyDataset(xx[:i],yy[:i], cfcf[:i])\n",
        "valset = MyDataset(xx[i:],yy[i:], cfcf[i:])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size, shuffle=True, pin_memory=True,num_workers=1)\n",
        "valid_loader = torch.utils.data.DataLoader(valset, batch_size, shuffle=False, pin_memory=True,num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v1LootliNNBl"
      },
      "outputs": [],
      "source": [
        "class modelF1(nn.Module):\n",
        "    \"\"\"\n",
        "    feature predictor\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(modelF1, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 2, kernel_size=3, padding=0),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(2, 4, kernel_size=3, padding=0),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(4, 8, kernel_size=3, padding=0),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class modelB1(nn.Module):\n",
        "    \"\"\"\n",
        "    bias predictor\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(modelB1, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Linear(32, 16),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(16, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class modelP1(nn.Module):\n",
        "    \"\"\"\n",
        "    predictor\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(modelP1, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Linear(32, 16),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20rg-ZmWNNBm",
        "outputId": "90d3c463-daa8-4c7c-fe8f-0319a1915d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available. Training on CPU\n"
          ]
        }
      ],
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available. Training on CPU')\n",
        "else:\n",
        "    print('CUDA is available. Training on GPU')\n",
        "\n",
        "device = torch.device(\"cuda\" if train_on_gpu else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rQakDhK9NNBm"
      },
      "outputs": [],
      "source": [
        "modelF = modelF1()\n",
        "modelP = modelP1()\n",
        "modelB = modelB1()\n",
        "\n",
        "modelF = torch.nn.DataParallel(modelF).to(device) # send tensor to device\n",
        "modelP = torch.nn.DataParallel(modelP).to(device) # send tensor to device\n",
        "modelB = torch.nn.DataParallel(modelB).to(device) # send tensor to device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RoE6fZ2dNNBm"
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.Adam(list(modelF.parameters())+ list(modelP.parameters())+list(modelB.parameters()), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "pNrLv_V5NNBm",
        "outputId": "4f6c1a41-675d-4cbe-d8df-3a8a5f3c2515"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e917e41c987e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# Step 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mlossp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlossb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#want to maximise lossb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# Step 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_loss_list = []\n",
        "valid_loss_list = []\n",
        "train_lossb_list = []\n",
        "valid_lossb_list = []\n",
        "\n",
        "\n",
        "\n",
        "epochs = 20000\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    train_lossb = 0.0\n",
        "    valid_lossb = 0.0\n",
        "\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    modelF.train()\n",
        "    modelP.train()\n",
        "    modelB.train()\n",
        "    for x, y, b in train_loader: # b is bias\n",
        "            #no control group as easier to train\n",
        "            x = x.permute(0,3,1,2)\n",
        "            y, b = torch.unsqueeze(y,1), torch.unsqueeze(b,1)\n",
        "            x,y,b = x.to(device), y.to(device), b.to(device)\n",
        "            F_pred = modelF(x)\n",
        "            y_pred = modelP(F_pred)\n",
        "            b_pred = modelB(F_pred)\n",
        "\n",
        "            #no adversarial loss for the ctrl group\n",
        "            lossp = lossp_calc(y_pred, y)\n",
        "            lossb = lossb_calc(b_pred, b)\n",
        "            lossb_adverseral = lossb_adverseral_calc(b_pred, b)\n",
        "\n",
        "            train_loss += lossp.item()\n",
        "            train_lossb += lossb.item()\n",
        "\n",
        "            opt.zero_grad(True)\n",
        "            # Step 1\n",
        "            (lossp - lossb).backward(retain_graph=True, inputs = tuple(modelF.parameters())) #want to maximise lossb\n",
        "\n",
        "            # Step 2\n",
        "            (lossb_adverseral).backward(retain_graph=True, inputs = tuple(modelB.parameters())) # want to minimise lossb_adverseral (b is - corr^2 or mse) this is adverseral part\n",
        "\n",
        "            # Step 3\n",
        "            (lossp).backward(inputs= tuple(modelP.parameters()))\n",
        "\n",
        "            opt.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    modelF.eval()\n",
        "    modelP.eval()\n",
        "    modelB.eval()\n",
        "    with torch.no_grad(): #to increase the validation process uses less memory\n",
        "        for x1, y1, b1 in valid_loader:\n",
        "            x1 = x1.permute(0,3,1,2)\n",
        "            y1, b1 = torch.unsqueeze(y1,1), torch.unsqueeze(b1,1)\n",
        "            x1,y1,b1 = x1.to(device), y1.to(device), b1.to(device)\n",
        "\n",
        "            F_pred = modelF(x1)\n",
        "            y_pred = modelP(F_pred)\n",
        "            b_pred = modelB(F_pred)\n",
        "\n",
        "            lossp = lossp_calc(y_pred, y1)\n",
        "            lossb = lossb_calc(b_pred, b1)\n",
        "\n",
        "            valid_loss += lossp.item()\n",
        "            valid_lossb += lossb.item()\n",
        "\n",
        "            valid_loss += lossp.item()\n",
        "            valid_lossb += lossb.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    valid_loss /= len(valid_loader)\n",
        "    train_lossb /= len(train_loader)\n",
        "    valid_lossb /= len(valid_loader) # hence this is the per batch loss\n",
        "\n",
        "\n",
        "    train_loss_list += [train_loss]\n",
        "    valid_loss_list += [valid_loss]\n",
        "    train_lossb_list += [train_lossb]\n",
        "    valid_lossb_list += [valid_lossb]\n",
        "\n",
        "\n",
        "    if (epoch%100==0):\n",
        "      #print time per epoch\n",
        "      time_elapsed = time.time() - since\n",
        "      print('Epoch {}/{}'.format(epoch+1, epochs),\n",
        "            'Train loss: {:.4f}'.format(train_loss),\n",
        "            'Valid loss: {:.4f}'.format(valid_loss),\n",
        "            'Train lossb: {:.4f}'.format(train_lossb),\n",
        "            'Valid lossb: {:.4f}'.format(valid_lossb),\n",
        "            'Time: {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEiDsJkfNNBm"
      },
      "outputs": [],
      "source": [
        "# Assuming you have the following variables defined:\n",
        "# train_loss, valid_loss, train_lossb, valid_lossb\n",
        "\n",
        "# Plotting loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(train_loss_list), label='Train Loss')\n",
        "plt.plot(np.array(valid_loss_list), label='Valid Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting lossb\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(train_lossb_list), label='Train Lossb')\n",
        "plt.plot(np.array(valid_lossb_list), label='Valid Lossb')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Lossb')\n",
        "plt.title('Lossb')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNWZcg11NNBn"
      },
      "source": [
        "# Visualizing the Association between Features and Confounder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIlsdmgNNBn"
      },
      "outputs": [],
      "source": [
        "for x in [train_x, valid_x]:\n",
        "\n",
        "  x = torch.tensor(x, dtype=torch.float32).to(device)\n",
        "  x = x.permute(0,3,1,2)\n",
        "\n",
        "\n",
        "  # Get the gradients of the output with respect to the input\n",
        "  x.requires_grad_()\n",
        "\n",
        "  modelF.eval()\n",
        "  modelP.eval()\n",
        "  output = modelP(modelF(x)).sum(dim=0)\n",
        "  modelF.zero_grad()\n",
        "  modelP.zero_grad()\n",
        "  output.backward()\n",
        "\n",
        "  # Compute the saliency map\n",
        "  saliency_map = x.grad.abs().squeeze().mean(dim=0).cpu().numpy()\n",
        "\n",
        "  # Normalize the saliency map\n",
        "  saliency_map /= saliency_map.max()\n",
        "\n",
        "  # Plot the original image and the saliency map\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(x[0,0,:,:].detach().cpu().numpy())\n",
        "  plt.title('Original Image')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(saliency_map)\n",
        "  plt.title('Saliency Map')\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9YcA_8h2_eSl"
      },
      "outputs": [],
      "source": [
        "# dcorr loss\n",
        "def dcor_loss_calc(X, Y):\n",
        "    \"\"\" Compute the distance correlation function using PyTorch tensors\"\"\"\n",
        "    #n = X.size(dim=1)\n",
        "\n",
        "    # Compute distance matrices\n",
        "    a = torch.cdist(X.unsqueeze(2), X.unsqueeze(2))\n",
        "    b = torch.cdist(Y.unsqueeze(2), Y.unsqueeze(2))\n",
        "    # assuming X, Y is a vector we unsqueeze\n",
        "\n",
        "\n",
        "\n",
        "    # Compute double centered distance matrices\n",
        "    A = a - a.mean(dim=1, keepdim=True) - a.mean(dim=2, keepdim=True) + a.mean(dim=(1,2), keepdim=True)\n",
        "    B = b - b.mean(dim=1, keepdim=True) - b.mean(dim=2, keepdim=True) + b.mean(dim=(1,2), keepdim=True)\n",
        "\n",
        "    # Compute distance covariance and variance\n",
        "    dCovXY = (A * B).sum(dim=(1,2)) #/ (n**2)\n",
        "    dVarX = (A**2).sum(dim=(1,2)) #/ (n**2)\n",
        "    dVarY = (B**2).sum(dim=(1,2)) #/ (n**2)\n",
        "\n",
        "    # Compute distance correlation\n",
        "    dCorXY = dCovXY / (dVarX * dVarY).sqrt()\n",
        "\n",
        "    return dCorXY.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Fkgi3qKS_eSl"
      },
      "outputs": [],
      "source": [
        "modelF_dcor = modelF1()\n",
        "modelP_dcor = modelP1()\n",
        "\n",
        "modelF_dcor = torch.nn.DataParallel(modelF_dcor).to(device) # send tensor to device\n",
        "modelP_dcor = torch.nn.DataParallel(modelP_dcor).to(device) # send tensor to device\n",
        "\n",
        "opt = torch.optim.Adam(list(modelF_dcor.parameters())+ list(modelP_dcor.parameters()), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YtTcPQYdNNBo",
        "outputId": "0d571f19-28b1-4232-dff0-1be72bdfe55d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.0397) tensor(0.2125)\n",
            "tensor(-0.2292, grad_fn=<MinBackward1>) tensor(0.1282, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0463) tensor(0.2265)\n",
            "tensor(-0.2288, grad_fn=<MinBackward1>) tensor(0.1276, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0416) tensor(0.2085)\n",
            "tensor(-0.2298, grad_fn=<MinBackward1>) tensor(0.1282, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0431) tensor(0.2194)\n",
            "tensor(-0.2297, grad_fn=<MinBackward1>) tensor(0.1288, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0472) tensor(0.2153)\n",
            "tensor(-0.2286, grad_fn=<MinBackward1>) tensor(0.1280, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0451) tensor(0.2129)\n",
            "tensor(-0.2289, grad_fn=<MinBackward1>) tensor(0.1294, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0413) tensor(0.2095)\n",
            "tensor(-0.2296, grad_fn=<MinBackward1>) tensor(0.1277, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0428) tensor(0.2180)\n",
            "tensor(-0.2297, grad_fn=<MinBackward1>) tensor(0.1287, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0405) tensor(0.2119)\n",
            "tensor(-0.2292, grad_fn=<MinBackward1>) tensor(0.1302, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0417) tensor(0.2141)\n",
            "tensor(-0.2286, grad_fn=<MinBackward1>) tensor(0.1289, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0430) tensor(0.2187)\n",
            "tensor(-0.2294, grad_fn=<MinBackward1>) tensor(0.1283, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0406) tensor(0.2083)\n",
            "tensor(-0.2288, grad_fn=<MinBackward1>) tensor(0.1283, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0444) tensor(0.2206)\n",
            "tensor(-0.2290, grad_fn=<MinBackward1>) tensor(0.1276, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "Epoch 1/20000 Train loss: 0.6755 Valid loss: 1.5721 Train lossb: nan Valid lossb: nan Time: 0m 1s\n",
            "tensor(-0.0444) tensor(0.2125)\n",
            "tensor(-0.2292, grad_fn=<MinBackward1>) tensor(0.1283, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0428) tensor(0.2078)\n",
            "tensor(-0.2293, grad_fn=<MinBackward1>) tensor(0.1275, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0403) tensor(0.2159)\n",
            "tensor(-0.2297, grad_fn=<MinBackward1>) tensor(0.1283, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0414) tensor(0.2119)\n",
            "tensor(-0.2298, grad_fn=<MinBackward1>) tensor(0.1302, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0472) tensor(0.2206)\n",
            "tensor(-0.2296, grad_fn=<MinBackward1>) tensor(0.1282, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0463) tensor(0.2265)\n",
            "tensor(-0.2288, grad_fn=<MinBackward1>) tensor(0.1294, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0430) tensor(0.2180)\n",
            "tensor(-0.2290, grad_fn=<MinBackward1>) tensor(0.1288, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0435) tensor(0.2143)\n",
            "tensor(-0.2286, grad_fn=<MinBackward1>) tensor(0.1276, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0420) tensor(0.2083)\n",
            "tensor(-0.2291, grad_fn=<MinBackward1>) tensor(0.1287, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0451) tensor(0.2194)\n",
            "tensor(-0.2292, grad_fn=<MinBackward1>) tensor(0.1279, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0403) tensor(0.2115)\n",
            "tensor(-0.2286, grad_fn=<MinBackward1>) tensor(0.1281, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0406) tensor(0.2106)\n",
            "tensor(-0.2290, grad_fn=<MinBackward1>) tensor(0.1289, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0437) tensor(0.2129)\n",
            "tensor(-0.2290, grad_fn=<MinBackward1>) tensor(0.1277, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0410) tensor(0.2153)\n",
            "tensor(-0.2288, grad_fn=<MinBackward1>) tensor(0.1281, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0416) tensor(0.2206)\n",
            "tensor(-0.2290, grad_fn=<MinBackward1>) tensor(0.1283, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0413) tensor(0.2201)\n",
            "tensor(-0.2288, grad_fn=<MinBackward1>) tensor(0.1282, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0444) tensor(0.2066)\n",
            "tensor(-0.2292, grad_fn=<MinBackward1>) tensor(0.1283, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0451) tensor(0.2180)\n",
            "tensor(-0.2298, grad_fn=<MinBackward1>) tensor(0.1271, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0428) tensor(0.2078)\n",
            "tensor(-0.2292, grad_fn=<MinBackward1>) tensor(0.1279, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0411) tensor(0.2265)\n",
            "tensor(-0.2287, grad_fn=<MinBackward1>) tensor(0.1276, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0472) tensor(0.2159)\n",
            "tensor(-0.2292, grad_fn=<MinBackward1>) tensor(0.1277, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0463) tensor(0.2096)\n",
            "tensor(-0.2297, grad_fn=<MinBackward1>) tensor(0.1302, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0403) tensor(0.2125)\n",
            "tensor(-0.2294, grad_fn=<MinBackward1>) tensor(0.1294, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0435) tensor(0.2194)\n",
            "tensor(-0.2296, grad_fn=<MinBackward1>) tensor(0.1288, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0420) tensor(0.2141)\n",
            "tensor(-0.2297, grad_fn=<MinBackward1>) tensor(0.1287, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0395) tensor(0.2053)\n",
            "tensor(-0.2286, grad_fn=<MinBackward1>) tensor(0.1274, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0435) tensor(0.2206)\n",
            "tensor(-0.2292, grad_fn=<MinBackward1>) tensor(0.1302, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0431) tensor(0.2072)\n",
            "tensor(-0.2285, grad_fn=<MinBackward1>) tensor(0.1270, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0444) tensor(0.2141)\n",
            "tensor(-0.2297, grad_fn=<MinBackward1>) tensor(0.1283, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0428) tensor(0.2141)\n",
            "tensor(-0.2291, grad_fn=<MinBackward1>) tensor(0.1282, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0403) tensor(0.2187)\n",
            "tensor(-0.2287, grad_fn=<MinBackward1>) tensor(0.1273, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0401) tensor(0.2180)\n",
            "tensor(-0.2294, grad_fn=<MinBackward1>) tensor(0.1283, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0403) tensor(0.2114)\n",
            "tensor(-0.2293, grad_fn=<MinBackward1>) tensor(0.1276, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0437) tensor(0.2115)\n",
            "tensor(-0.2297, grad_fn=<MinBackward1>) tensor(0.1277, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0416) tensor(0.2119)\n",
            "tensor(-0.2288, grad_fn=<MinBackward1>) tensor(0.1294, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0428) tensor(0.2159)\n",
            "tensor(-0.2298, grad_fn=<MinBackward1>) tensor(0.1282, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0417) tensor(0.2265)\n",
            "tensor(-0.2290, grad_fn=<MinBackward1>) tensor(0.1288, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0472) tensor(0.2201)\n",
            "tensor(-0.2296, grad_fn=<MinBackward1>) tensor(0.1274, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0451) tensor(0.2153)\n",
            "tensor(-0.2287, grad_fn=<MinBackward1>) tensor(0.1279, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0444) tensor(0.2115)\n",
            "tensor(-0.2292, grad_fn=<MinBackward1>) tensor(0.1274, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0430) tensor(0.2201)\n",
            "tensor(-0.2292, grad_fn=<MinBackward1>) tensor(0.1288, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0428) tensor(0.2187)\n",
            "tensor(-0.2285, grad_fn=<MinBackward1>) tensor(0.1271, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0472) tensor(0.2265)\n",
            "tensor(-0.2296, grad_fn=<MinBackward1>) tensor(0.1276, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0435) tensor(0.2065)\n",
            "tensor(-0.2291, grad_fn=<MinBackward1>) tensor(0.1279, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0403) tensor(0.2159)\n",
            "tensor(-0.2297, grad_fn=<MinBackward1>) tensor(0.1287, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0437) tensor(0.2119)\n",
            "tensor(-0.2292, grad_fn=<MinBackward1>) tensor(0.1294, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0416) tensor(0.2119)\n",
            "tensor(-0.2285, grad_fn=<MinBackward1>) tensor(0.1283, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0451) tensor(0.2153)\n",
            "tensor(-0.2298, grad_fn=<MinBackward1>) tensor(0.1276, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0393) tensor(0.2141)\n",
            "tensor(-0.2290, grad_fn=<MinBackward1>) tensor(0.1302, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0411) tensor(0.2206)\n",
            "tensor(-0.2297, grad_fn=<MinBackward1>) tensor(0.1289, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0431) tensor(0.2134)\n",
            "tensor(-0.2293, grad_fn=<MinBackward1>) tensor(0.1276, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0401) tensor(0.2106)\n",
            "tensor(-0.2287, grad_fn=<MinBackward1>) tensor(0.1282, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4555, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0403) tensor(0.2187)\n",
            "tensor(-0.2298, grad_fn=<MinBackward1>) tensor(0.1294, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0381) tensor(0.2119)\n",
            "tensor(-0.2288, grad_fn=<MinBackward1>) tensor(0.1280, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0401) tensor(0.2194)\n",
            "tensor(-0.2292, grad_fn=<MinBackward1>) tensor(0.1271, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0413) tensor(0.2075)\n",
            "tensor(-0.2288, grad_fn=<MinBackward1>) tensor(0.1280, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0472) tensor(0.2201)\n",
            "tensor(-0.2290, grad_fn=<MinBackward1>) tensor(0.1302, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0431) tensor(0.2265)\n",
            "tensor(-0.2286, grad_fn=<MinBackward1>) tensor(0.1283, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0414) tensor(0.2129)\n",
            "tensor(-0.2296, grad_fn=<MinBackward1>) tensor(0.1271, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0416) tensor(0.2141)\n",
            "tensor(-0.2289, grad_fn=<MinBackward1>) tensor(0.1277, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0444) tensor(0.2141)\n",
            "tensor(-0.2297, grad_fn=<MinBackward1>) tensor(0.1282, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0463) tensor(0.2159)\n",
            "tensor(-0.2297, grad_fn=<MinBackward1>) tensor(0.1282, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0430) tensor(0.2206)\n",
            "tensor(-0.2291, grad_fn=<MinBackward1>) tensor(0.1288, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0451) tensor(0.2099)\n",
            "tensor(-0.2294, grad_fn=<MinBackward1>) tensor(0.1287, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4554, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0428) tensor(0.2072)\n",
            "tensor(-0.2285, grad_fn=<MinBackward1>) tensor(0.1283, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0444) tensor(0.2106)\n",
            "tensor(-0.2292, grad_fn=<MinBackward1>) tensor(0.1283, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0413) tensor(0.2114)\n",
            "tensor(-0.2290, grad_fn=<MinBackward1>) tensor(0.1279, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0451) tensor(0.2206)\n",
            "tensor(-0.2297, grad_fn=<MinBackward1>) tensor(0.1302, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4562, grad_fn=<MaxBackward1>)\n",
            "tensor(-0.0420) tensor(0.2159)\n",
            "tensor(-0.2298, grad_fn=<MinBackward1>) tensor(0.1280, grad_fn=<MaxBackward1>)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.4553, grad_fn=<MinBackward1>) tensor(0.4563, grad_fn=<MaxBackward1>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-7971cd3119d9>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mlossp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlossb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lam = 1\n",
        "\n",
        "epochs = 20000\n",
        "\n",
        "\n",
        "train_loss_list = []\n",
        "valid_loss_list = []\n",
        "train_lossb_list = []\n",
        "valid_lossb_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    train_lossb = 0.0\n",
        "    valid_lossb = 0.0\n",
        "\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    modelF_dcor.train()\n",
        "    modelP_dcor.train()\n",
        "    for x, y, b in train_loader: # b is bias\n",
        "            #no control group as easier to train\n",
        "            x,y,b = x.to(device), y.to(device), b.to(device)\n",
        "            x = x.permute(0,3,1,2)\n",
        "            y, b = torch.unsqueeze(y,1), torch.unsqueeze(b,1)\n",
        "            F_pred = modelF_dcor(x)\n",
        "            y_pred = modelP_dcor(F_pred)\n",
        "\n",
        "\n",
        "            #no adversarial loss for the ctrl group\n",
        "            lossp = lossp_calc(y_pred, y)\n",
        "            lossb = dcor_loss_calc(F_pred, b)\n",
        "\n",
        "            train_loss += lossp.item()\n",
        "            train_lossb += lossb.item()\n",
        "\n",
        "            opt.zero_grad(True)\n",
        "            (lossp + lam*lossb).backward()\n",
        "            opt.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    modelF_dcor.eval()\n",
        "    modelP_dcor.eval()\n",
        "    with torch.no_grad(): #to increase the validation process uses less memory\n",
        "        for x1, y1, b1 in valid_loader:\n",
        "            x1,y1,b1 = x1.to(device), y1.to(device), b1.to(device)\n",
        "            x1 = x1.permute(0,3,1,2)\n",
        "            y1, b1 = torch.unsqueeze(y1,1), torch.unsqueeze(b1,1)\n",
        "\n",
        "            F_pred = modelF_dcor(x1)\n",
        "            y_pred = modelP_dcor(F_pred)\n",
        "\n",
        "            lossp = lossp_calc(y_pred, y1)\n",
        "            lossb = dcor_loss_calc(F_pred, b1)\n",
        "\n",
        "            valid_loss += lossp.item()\n",
        "            valid_lossb += lossb.item()\n",
        "\n",
        "            valid_loss += lossp.item()\n",
        "            valid_lossb += lossb.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    valid_loss /= len(valid_loader)\n",
        "    train_lossb /= len(train_loader)\n",
        "    valid_lossb /= len(valid_loader) # hence this is the per batch loss\n",
        "\n",
        "\n",
        "    train_loss_list += [train_loss]\n",
        "    valid_loss_list += [valid_loss]\n",
        "    train_lossb_list += [train_lossb]\n",
        "    valid_lossb_list += [valid_lossb]\n",
        "\n",
        "\n",
        "    if (epoch%100==0):\n",
        "      #print time per epoch\n",
        "      time_elapsed = time.time() - since\n",
        "      print('Epoch {}/{}'.format(epoch+1, epochs),\n",
        "            'Train loss: {:.4f}'.format(train_loss),\n",
        "            'Valid loss: {:.4f}'.format(valid_loss),\n",
        "            'Train lossb: {:.4f}'.format(train_lossb),\n",
        "            'Valid lossb: {:.4f}'.format(valid_lossb),\n",
        "            'Time: {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ2tkLBx_eSm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Plotting loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(train_loss_list), label='Train Loss')\n",
        "plt.plot(np.array(valid_loss_list), label='Valid Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting lossb\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(train_lossb_list), label='Train Lossb')\n",
        "plt.plot(np.array(valid_lossb_list), label='Valid Lossb')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Lossb')\n",
        "plt.title('Lossb')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in [train_x, valid_x]:\n",
        "\n",
        "  x = torch.tensor(x, dtype=torch.float32).to(device)\n",
        "  x = x.permute(0,3,1,2)\n",
        "\n",
        "\n",
        "  # Get the gradients of the output with respect to the input\n",
        "  x.requires_grad_()\n",
        "\n",
        "  modelF.eval()\n",
        "  modelP.eval()\n",
        "  output = modelP(modelF(x)).sum(dim=0)\n",
        "  modelF.zero_grad()\n",
        "  modelP.zero_grad()\n",
        "  output.backward()\n",
        "\n",
        "  # Compute the saliency map\n",
        "  saliency_map = x.grad.abs().squeeze().mean(dim=0).cpu().numpy()\n",
        "\n",
        "  # Normalize the saliency map\n",
        "  saliency_map /= saliency_map.max()\n",
        "\n",
        "  # Plot the original image and the saliency map\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(x[0,0,:,:].detach().cpu().numpy())\n",
        "  plt.title('Original Image')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(saliency_map)\n",
        "  plt.title('Saliency Map')\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "kaduSmmgE9ru"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}